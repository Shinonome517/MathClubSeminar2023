\documentclass{classes/myslide}

\title{情報源符号化定理を示そう}
\author{溝口稜太}
\institute{創域理工学部情報計算科学科４年}
\date{September 24, 2023}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}\frametitle{イントロダクション（圧縮の例）}

  「情報源符号化定理」→"圧縮の限界"を示す定理

  \begin{exampleblock}{圧縮の例}
    \begin{align*}
      &000100000110000 \\
      \xrightarrow[圧縮]{} & 03 \ 11 \ 05 \ 12 \ 04 \\
      \xrightarrow[復元]{} & 000100000110000 
    \end{align*}
  \end{exampleblock}

  \begin{alertblock}{気づくこと}
    出現頻度の偏りが大きければ、より圧縮ができそう
  \end{alertblock}

\end{frame}

\begin{frame}\frametitle{数学的準備（圧縮のモデル）}
  「文字の生成」のモデルに必要な数学的定義を行う
  \begin{definition}[情報源など]
    $\mathcal{X}$：情報源 \ （有限集合）\\
    $X$：$\mathcal{X}$値離散確率変数 \ （可測写像：$\mathrm{prob. sp.} \rightarrow \mathcal{X}$）\\
    $x$：シンボル, $X$の実現値 \ （$ x \in \mathcal{X}$） \\
    $P_X(\{x\})$：$X$の分布 \ （$(\mathcal{X}, 2^{X})$上の確率測度$P_X: \mathcal{X} \rightarrow [0, 1]$） \\
    $p(x)$：$X$の確率関数, 分布$P_X(\{x\})$の確率関数 \ （$p(x) \coloneq P_X(\{x\})$）
  \end{definition}
\end{frame}

\begin{frame}\frametitle{数学的準備（圧縮のモデル）}
  「文字列の生成と圧縮・復元」のモデルに必要な数学的定義を行う
  \begin{definition}[拡大情報源など]
    $n, m$：シンボル列長,ビット列長 \ （$n, m \in \mathbb{N}$）\\
    $\mathcal{X}^n$：拡大情報源 \ （$\mathcal{X}$の$n$個の直積）\\
    $(X_1, X_2, ..., X_n)$：$\mathcal{X}^n$値離散確率変数 \ （$\mathcal{X}^n$値確率変数の列）\\
    $(x_1, x_2, ..., x_n )$：シンボル列, $(X_1, X_2, ..., X_n)$の実現値  \\ \quad （$ x \in \mathcal{X}$の列）\\
    $\tilde{p}(x_1, x_2, ..., x_n )$：$(X_1, X_2, ..., X_n)$の確率関数 \\ \quad（$\tilde{p}(x_1, x_2, ..., x_n ) \coloneq P(X_1 = x_1, X_2 = x_2,... X_n = x_n)$）
  \end{definition}
\end{frame}

\begin{frame}\frametitle{数学的準備（圧縮のモデル）}
  続き
  \begin{definition}[拡大情報源など]
    $\{0, 1\}^n$：符号源 \\
    $(y_1, y_2,... y_n)$：ビット列 \ （$y \in \{0, 1\}$の列）\\
    $C(x_1, x_2, ..., x_n )$：符号化関数 \ （関数：$\mathcal{X}^n \rightarrow \{0, 1\}^n$）\\
    $l(y_1, y_2, ..., y_m )$：符号語長関数 \ （関数：$\{0, 1\}^n \rightarrow \mathbb{N}$） \\
    $D(y_1, y_2, ..., y_m )$：復号化関数  \ （関数：$\{0, 1\}^n \rightarrow \mathcal{X}^n$） \\
    $R$：圧縮率 \ （$R \coloneq E[\frac{1}{n}l(C((X_1, X_2, ..., X_n)))] \in [0, 1]$）
  \end{definition}
\end{frame}

\begin{frame}\frametitle{数学的準備（圧縮のモデル）}
  "適切な圧縮"を定義する
  \begin{definition}[可逆な圧縮]
    符号化関数$C$が以下を満たすとき
    \begin{multline*}
      \forall n \in \mathbb{N}, \forall (x_1, x_2, ..., x_n ) \in \mathcal{X}^n, \exists D; \\ D(C(x_1, x_2, ..., x_n )) = (x_1, x_2, ..., x_n )
    \end{multline*}
    この$C$による圧縮を可逆な圧縮という
  \end{definition}
  \begin{alertblock}{考えたいこと}
    可逆な圧縮の最大効率
  \end{alertblock}
\end{frame}

\begin{frame}\frametitle{数学的準備（エントロピー）}
  情報源の分布（$\fallingdotseq$確率関数）に対して「エントロピー」という量を定義する
  \begin{definition}[エントロピー]
    $\mathcal{X}$：情報源； \quad $X$：$\mathcal{X}$値離散確率変数 \\
    このとき、以下で定義する$H(X)$をエントロピー（平均情報量）という
    \begin{align*}
      H(X) 
      &\coloneq E[-\log p(X)] \\ 
      &= -\sum_{x \in \mathcal{X}}p(x) \log p(x)
    \end{align*}
    但し，$0 \log 0 = 0$とし，対数の底は$2$とする
  \end{definition}
\end{frame}

\begin{frame}\frametitle{数学的準備（エントロピー）}
  \begin{exampleblock}{エントロピーの例}
    \[ \mathcal{X} = \{a, b, c\} \]
    \[
      \begingroup
      \renewcommand{\arraystretch}{1.2}
        \begin{array}{c|c|c|c}
          x & a & b & c \\ \hline
          p(x) & \frac{1}{6} & \frac{1}{3} & \frac{1}{2}
        \end{array}
      \endgroup
    \]
    \begin{align*}
      H(X)
      &\coloneq - \left( \frac{1}{6} \log \frac{1}{6} + \frac{1}{3} \log \frac{1}{3} + \frac{1}{2} \log \frac{1}{2}\right) \\
      &= \frac{1}{2}\log 3 + \frac{2}{3} \\
      &\simeq 1.459147917027244
    \end{align*}
  \end{exampleblock}
\end{frame}

\begin{frame}\frametitle{数学的準備（エントロピー）}
  \begin{exampleblock}{エントロピーの例（最小値）}
    \[ \mathcal{X} = \{a, b, c\} \]
    \[
      \begingroup
      \renewcommand{\arraystretch}{1.2}
        \begin{array}{c|c|c|c}
          x & a & b & c \\ \hline
          p(x) & 0 & 0 & 1
        \end{array}
      \endgroup
    \]
    \begin{align*}
      H(X)
      &\coloneq -(0 \log 0 + 0 \log 0 + 1 \log 1) \\
      &= 0
    \end{align*}
  \end{exampleblock}
\end{frame}

\begin{frame}\frametitle{数学的準備（エントロピー）}
  エントロピーの最大値を考える
  \begin{theorem}[エントロピーの最大値]
    一様分布がエントロピーの最大値を与える
    \[H(X) \leq \log |\mathcal{X}| \]
  \end{theorem}
  \begin{proof}[証明]
    $\mathcal{X}$は有限集合であるから$|\mathcal{X}| = n \in \mathbb{N}$と置ける．

    すると一様分布の確率関数は$p(x) = \frac{1}{n}$と書けるので，そのエントロピーは
      \[ - \sum_{i = 1}^{n} \frac{1}{n} \log \frac{1}{n} = \log n \]
    である．以下，$p(x)$を任意に固定した分布の確率関数として，
    \let\qedsymbol\relax
  \end{proof}
\end{frame}

\begin{frame}\frametitle{数学的準備（エントロピー）}
  \begin{proof}[証明（続き）]
    \begin{align*}
      \log n - H(X) 
      &= -\sum_{i = 1}^{n} p(x_i) \log \frac{1}{n} + \sum_{i = 1}^{n} p(x_i) \log p(x_i) \\
      &= -\sum_{i = 1}^n p(x_i) \log \frac{\frac{1}{n}}{p(x_i)} \\
      &\geq \sum_{i = 1}^n p(x_i) \left( 1 - \frac{\frac{1}{n}}{p(x_i)}\right)\quad \because x - 1 \leq \log x \\
      &= \sum_{i = 1}^n p(x_i) - \sum_{i = 1}^{n} \frac{1}{n} \\
      &= 0
    \end{align*}
  \end{proof}
\end{frame}

\begin{frame}\frametitle{数学的準備（エントロピー）}
  \begin{alertblock}{エントロピーに対する考察}
    \begin{itemize}
      \item 分布のあいまいさが最小のとき→エントロピーは最小値をとる
      \item 分布のあいまいさが最大のとき→エントロピーは最大値をとる
    \end{itemize}
    ⇒エントロピーは分布に対して定義される．曖昧さの尺度（測度）である．
  \end{alertblock}
\end{frame}

\end{document}
