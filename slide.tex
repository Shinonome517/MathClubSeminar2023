\documentclass{classes/myslide}

\title{情報源符号化定理を示そう}
\author{溝口稜太}
\institute{創域理工学部情報計算科学科４年}
\date{September 24, 2023}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}\frametitle{イントロダクション（圧縮の例）}

  「情報源符号化定理」とは"圧縮の限界"を示す定理である．

  \begin{exampleblock}{圧縮の例}
    \begin{align*}
      &000100000110000 \\
      \xrightarrow[圧縮]{} & 03 \ 11 \ 05 \ 12 \ 04 \\
      \xrightarrow[復元]{} & 000100000110000 
    \end{align*}
  \end{exampleblock}

  \begin{alertblock}{気づくこと}
    出現頻度の偏りが大きければ、より圧縮ができそう
  \end{alertblock}

\end{frame}

\begin{frame}\frametitle{数学的準備（圧縮のモデル）}
  「文字の生成」のモデルに必要な数学的定義を行う．
  \begin{definition}[情報源など]
    $\mathcal{X}$：情報源 \ （有限集合）\\
    $X$：$\mathcal{X}$値離散確率変数 \ （可測写像：$\mathrm{prob. sp.} \rightarrow \mathcal{X}$）\\
    $x$：シンボル, $X$の実現値 \ （$ x \in \mathcal{X}$） \\
    $P_X(\{x\})$：$X$の分布 \ （$(\mathcal{X}, 2^{X})$上の確率測度$P_X: \mathcal{X} \rightarrow [0, 1]$） \\
    $p(x)$：$X$の確率関数, 分布$P_X(\{x\})$の確率関数 \ （$p(x) \coloneq P_X(\{x\})$）
  \end{definition}
\end{frame}

\begin{frame}\frametitle{数学的準備（圧縮のモデル）}
  「文字列の生成と圧縮・復元」のモデルに必要な数学的定義を行う．
  \begin{definition}[拡大情報源など]
    $n, m$：シンボル列長,ビット列長 \ （$n, m \in \mathbb{N}$）\\
    $\mathcal{X}^n$：拡大情報源 \ （$\mathcal{X}$の$n$個の直積）\\
    $(X_1, X_2, ..., X_n)$：$\mathcal{X}^n$値離散確率変数 \ （$\mathcal{X}^n$値確率変数の列）\\
    $(x_1, x_2, ..., x_n )$：シンボル列, $(X_1, X_2, ..., X_n)$の実現値  \\ \quad （$ x \in \mathcal{X}$の列）\\
    $P_{(X_1, X_2, ..., X_n)}(\{(x_1, x_2, ..., x_n )\})$：$(X_1, X_2, ..., X_n)$の分布 \\ \quad $(\mathcal{X}^n, 2^{{X}^n})$上の確率測度$P_{(X_1, X_2, ..., X_n)}: \mathcal{X}^n \rightarrow [0, 1]$） \\
    $\tilde{p}(x_1, x_2, ..., x_n )$：$(X_1, X_2, ..., X_n)$の確率関数 \\ \quad（$\tilde{p}(x_1, x_2, ..., x_n ) \coloneq P(\{(x_1, x_2, ..., x_n )\})$）
  \end{definition}
\end{frame}

\begin{frame}\frametitle{数学的準備（圧縮のモデル）}
  続き
  \begin{definition}[拡大情報源など]
    $\{0, 1\}^n$：符号源 \\
    $(y_1, y_2,... y_n)$：ビット列 \ （$y \in \{0, 1\}$の列）\\
    $C(x_1, x_2, ..., x_n )$：符号化関数 \ （関数：$\mathcal{X}^n \rightarrow \{0, 1\}^n$）\\
    $l(y_1, y_2, ..., y_m )$：符号語長関数 \ （関数：$\{0, 1\}^n \rightarrow \mathbb{N}$） \\
    $D(y_1, y_2, ..., y_m )$：復号化関数  \ （関数：$\{0, 1\}^n \rightarrow \mathcal{X}^n$） \\
    $R$：圧縮率 \ （$R \coloneq E[\frac{1}{n}l(C((X_1, X_2, ..., X_n)))] \in \mathbb{R}_{\geq 0}$）
  \end{definition}
\end{frame}

\begin{frame}\frametitle{数学的準備（圧縮のモデル）}
  "適切な圧縮"を定義する．
  \begin{definition}[可逆な圧縮]
    符号化関数$C$が以下を満たすとき
    \begin{multline*}
      \forall n \in \mathbb{N}, \forall (x_1, x_2, ..., x_n ) \in \mathcal{X}^n, \exists D; \\ D(C(x_1, x_2, ..., x_n )) = (x_1, x_2, ..., x_n )
    \end{multline*}
    この$C$による圧縮を可逆な圧縮という．
  \end{definition}
  \begin{alertblock}{考えたいこと}
    可逆な圧縮の最大効率
  \end{alertblock}
\end{frame}

\begin{frame}\frametitle{数学的準備（エントロピー）}
  情報源の分布（$\fallingdotseq$確率関数）に対して「エントロピー」という量を定義する．
  \begin{definition}[エントロピー]
    $\mathcal{X}$：情報源 \\
    $X$：$\mathcal{X}$値離散確率変数 \\
    $p(x)$：$X$の確率関数 \\
    このとき，以下で定義する$H(X)$をエントロピー（平均情報量）という．
    \begin{align*}
      H(X) 
      &\coloneq E[-\log p(X)] \\ 
      &= -\sum_{x \in \mathcal{X}}p(x) \log p(x)
    \end{align*}
    但し，連続性の議論から$0 \log 0 = 0$とし，対数の底は$2$とする．
  \end{definition}
\end{frame}

\begin{frame}\frametitle{数学的準備（エントロピー）}
  \begin{exampleblock}{エントロピーの例}
    \[ \mathcal{X} = \{a, b, c\} \]
    \[
      \begingroup
      \renewcommand{\arraystretch}{1.2}
        \begin{array}{c|c|c|c}
          x & a & b & c \\ \hline
          p(x) & \frac{1}{6} & \frac{1}{3} & \frac{1}{2}
        \end{array}
      \endgroup
    \]
    \begin{align*}
      H(X)
      &\coloneq - \left( \frac{1}{6} \log \frac{1}{6} + \frac{1}{3} \log \frac{1}{3} + \frac{1}{2} \log \frac{1}{2}\right) \\
      &= \frac{1}{2}\log 3 + \frac{2}{3} \\
      &\simeq 1.459147917027244
    \end{align*}
  \end{exampleblock}
\end{frame}

\begin{frame}\frametitle{数学的準備（エントロピー）}
  \begin{exampleblock}{エントロピーの例（最小値）}
    \[ \mathcal{X} = \{a, b, c\} \]
    \[
      \begingroup
      \renewcommand{\arraystretch}{1.2}
        \begin{array}{c|c|c|c}
          x & a & b & c \\ \hline
          p(x) & 0 & 0 & 1
        \end{array}
      \endgroup
    \]
    \begin{align*}
      H(X)
      &\coloneq -(0 \log 0 + 0 \log 0 + 1 \log 1) \\
      &= 0
    \end{align*}
  \end{exampleblock}
\end{frame}

\begin{frame}\frametitle{数学的準備（エントロピー）}
  エントロピーの最大値を考える
  \begin{theorem}[エントロピーの最大値]
    一様分布がエントロピーの最大値を与える．
    \[H(X) \leq \log |\mathcal{X}| \]
  \end{theorem}
  \begin{proof}[証明]
    $\mathcal{X}$は有限集合であるから$|\mathcal{X}| = n \in \mathbb{N}$と置ける．

    すると一様分布の確率関数は$p(x) = \frac{1}{n}$と書けるので，そのエントロピーは
      \[ - \sum_{i = 1}^{n} \frac{1}{n} \log \frac{1}{n} = \log n \]
    である．以下，$p(x)$を任意に固定した分布の確率関数として，
    \let\qedsymbol\relax
  \end{proof}
\end{frame}

\begin{frame}\frametitle{数学的準備（エントロピー）}
  \begin{proof}[証明（続き）]
    \begin{align*}
      \log n - H(X) 
      &= -\sum_{i = 1}^{n} p(x_i) \log \frac{1}{n} + \sum_{i = 1}^{n} p(x_i) \log p(x_i) \\
      &= -\sum_{i = 1}^n p(x_i) \log \frac{\frac{1}{n}}{p(x_i)} \\
      &\geq \sum_{i = 1}^n p(x_i) \left( 1 - \frac{\frac{1}{n}}{p(x_i)}\right)\quad \because x - 1 \leq \log x \\
      &= \sum_{i = 1}^n p(x_i) - \sum_{i = 1}^{n} \frac{1}{n} \\
      &= 0
    \end{align*}
  \end{proof}
\end{frame}

\begin{frame}\frametitle{数学的準備（エントロピー）}
  \begin{alertblock}{エントロピーに対する考察}
    \begin{itemize}
      \item 分布の"あいまいさ"が最小のとき→エントロピーは最小値をとる
      \item 分布の"あいまいさ"が最大のとき→エントロピーは最大値をとる
    \end{itemize}
    ⇒エントロピーは分布に対して定義される．"あいまいさ"の尺度（測度）である．
  \end{alertblock}
\end{frame}

\begin{frame}\frametitle{情報源符号化定理}
  \begin{theorem}[情報源符号化定理]
    $\mathcal{X}$：情報源 \\
    $X$：$\mathcal{X}$値離散確率変数 \\
    $p(x)$：$X$の確率関数 \\
    $r \in \mathbb{R}_{\geq 0}$：適当な実数 \\
    \begin{itemize}
      \item $r > H(X)$のとき \\ 拡大情報源から生成された任意のシンボル列に対し，圧縮率$r$の可逆な圧縮が存在する．
      \item $r < H(X)$のとき \\ 上記のような圧縮率$r$の可逆な圧縮は存在しない．
    \end{itemize}
  \end{theorem}
  ⇒この定理は 
  \begin{quote}
    "十分大きな$n$で限りなく$1$に近い確率で生成される一部のシンボル列" \ にのみ符号を割り当てる
  \end{quote}
  というアイデアに基づいて証明される．
\end{frame}

\begin{frame}\frametitle{$\varepsilon$-典型列と$\varepsilon$-典型集合}
  これ以降，次の略記を用いる．
  \begin{align*}
    X^n &\coloneq  (X_1, X_2, ..., X_n) \\
    x^n &\coloneq (x_1, x_2, ..., x_n )
  \end{align*}
  \begin{definition}[$\varepsilon$-典型列と$\varepsilon$-典型集合]
    $\mathcal{X}$：情報源； \quad  $X$：$\mathcal{X}$値離散確率変数； \quad $p(x)$：$X$の確率関数 \\
    $\mathcal{X}^n$：拡大情報源： \  $X^n$：$\mathcal{X}^n$値確率変数； \  $\tilde{p}(x^n)$：$X^n$の確率関数

    $n \in \mathbb{N}$個のシンボルからなるシンボル列$x^n$を考える．これが，
    \[
      \forall \varepsilon > 0, \  2^{-n(H(X) + \varepsilon)} \leq \tilde{p}(x^n) \leq  2^{-n(H(X) - \varepsilon)} 
    \]
    を満たすとき，これを「確率関数$\tilde{p}(x^n)$に関する$\varepsilon$-典型列」という．

    また，その列の集まりを「確率関数$\tilde{p}(x^n)$に関する$\varepsilon$-典型集合」といい
    $A_{\varepsilon}^{(n)}$とかく．
  \end{definition}
\end{frame}

\begin{frame}\frametitle{$\varepsilon$-典型列と$\varepsilon$-典型集合}
  \begin{theorem}[典系列の性質]
    以下，$\varepsilon > 0$は任意にとる．
    \begin{enumerate}
      \item $x^n \in A_{\varepsilon}^{(n)}$に対し， $H(X) - \varepsilon \leq -\frac{1}{n} \log \tilde{p}(x^n) \leq H(X) + \varepsilon$
      \item 十分大きな$n$に対し，$P_{X^n}(A_{\varepsilon}^{(n)}) > 1 - \varepsilon$
      \item $|A_{\varepsilon}^{(n)}| \leq 2^{n(H(X) + \varepsilon)}$
      \item 十分大きな$n$に対し，$|A_{\varepsilon}^{(n)}| \geq (1 - \varepsilon)2^{n(H(X) - \varepsilon)}$
    \end{enumerate}
  \end{theorem}
  \begin{proof}[証明]
    1は定義より自明
    \let\qedsymbol\relax
  \end{proof}
\end{frame}

\begin{frame}\frametitle{$\varepsilon$-典型列と$\varepsilon$-典型集合}
  \begin{proof}[続き]
    2について,
    \begin{multline*}
      P_{X^n}(A_{\varepsilon}^{(n)}) = P\lparen (X_1, X_2, ... X_n) \in A_{\varepsilon}^{(n)} \rparen  \\ 
        = P \left \lparen \left | -\frac{1}{n} \log \tilde{p}(X_1, X_2, ..., X_n) - H(X) \right | \leq \varepsilon \right \rparen  \quad \because \text{典系列の定義}
    \end{multline*}
    ここで大数の弱法則より，任意の$\delta > 0$に対し，十分大きな$n$において，
    \[
      P \left \lparen \left | -\frac{1}{n} \log \tilde{p}(X_1, X_2, ..., X_n) - H(X) \right | \leq \varepsilon \right \rparen > 1 - \delta
    \]
    $\varepsilon = \delta$とすれば，示すべきを得る．
    \let\qedsymbol\relax
  \end{proof}
\end{frame}

\begin{frame}\frametitle{$\varepsilon$-典型列と$\varepsilon$-典型集合}
  \begin{proof}[続き]
    3に関して，
    \begin{align*}
      1 &= \sum_{x^n \in \mathcal{X}^n} \tilde{p}(x^n) \\
        &\geq \sum_{x^n \in A_{\varepsilon}^{(n)}} \tilde{p}(x^n)  \\
        &\geq \sum_{x^n \in A_{\varepsilon}^{(n)}} 2^{-n(H(X) + \varepsilon)} \quad \because \text{典系列の定義} \\
        &= |A_{\varepsilon}^{(n)}| 2^{-n(H(X) + \varepsilon)}
    \end{align*}
    これより，
    \[
      |A_{\varepsilon}^{(n)}| \leq 2^{n(H(X) + \varepsilon)}
    \]
    \let\qedsymbol\relax
  \end{proof}
\end{frame}

\begin{frame}\frametitle{$\varepsilon$-典型列と$\varepsilon$-典型集合}
  \begin{proof}[続き]
    最後4について，2より十分大きな$n$に対して，
    \begin{align*}
      1 - \varepsilon 
      &< P_{X^n}(A_{\varepsilon}^{(n)}) \\
      &= \sum_{x^n \in A_{\varepsilon}^{(n)}} \tilde{p}(x^n) \\
      &\leq \sum_{x^n \in A_{\varepsilon}^{(n)}} 2^{-n(H(X) - \varepsilon)} \quad \because \text{典系列の定義} \\
      &= |A_{\varepsilon}^{(n)}| 2^{-n(H(X) - \varepsilon)}
    \end{align*}
    これより，
    \[
      |A_{\varepsilon}^{(n)}| \geq (1 - \varepsilon)2^{n(H(X) - \varepsilon)}
    \]
    を得る．
  \end{proof}
\end{frame}


\begin{frame}\frametitle{情報源符号化定理の証明}
  \begin{proof}[証明]
    $r > H(X)$のとき

    $H(X) + \varepsilon < r$ を満たすように，任意に$\varepsilon > 0$ を固定できる．

    ここでその$\varepsilon$に対する$\varepsilon$-典型集合$A_{\varepsilon}^{(n)}$を考える．

    $\varepsilon$-典型集合の要素数は，
    \[ |A_{\varepsilon}^{(n)}| \leq 2^{n(H(X) + \varepsilon)}  \leq 2^{nr} \leq 2^{\lfloor nr \rfloor} + 1\]
    と$r$を用いて評価できる．さらに，十分大きな$n$に対して，
    \[ P_{X^n}(A_{\varepsilon}^{(n)}) > 1 - \varepsilon \]
    が成立する．すなわち，$n$を大きくすることで，シンボル列が典型列である確率をいくらでも$1$に近づけることができる．
    \let\qedsymbol\relax
  \end{proof}
\end{frame}

\begin{frame}\frametitle{情報源符号化定理の証明}
  \begin{proof}[続き]
    そのため，
    \[ 
      C(x^n) = 
        \begin{cases}
          y_m \in \{0, 1\}^m \quad (m <\lfloor nr \rfloor + 1 \textbf{で一意}) &  (x^n \in A_{\varepsilon}^{(n)})\\
          1^{\lfloor nr \rfloor + 2} & (x^n \in (A_{\varepsilon}^{(n)})^c)
        \end{cases}
    \]
    と符号化関数を定めることができ，これは$n \to \infty$の極限において可逆である．
    この圧縮操作の圧縮率$R$は

    \begin{align*}
      R 
      &= \mathrm{E}\left \lbrack \frac{1}{n} l(C(X^n)) \right \rbrack  \\
      &= \frac{1}{n} \sum_{x^n \in \mathcal{X}^n} \tilde{p}(x^n)l(x^n) 
    \end{align*}
    \let\qedsymbol\relax
  \end{proof}
\end{frame}

\begin{frame}\frametitle{情報源符号化定理の証明}
  \begin{proof}[続き]
    \begin{align*}
      &= \frac{1}{n} \Bigg \lparen \sum_{x^n \in A_{\varepsilon}^{(n)}} \tilde{p}(x^n)l(C(x^n)) 
        + \sum_{x^n \in (A_{\varepsilon}^{(n)})^c} \tilde{p}(x^n)l(C(x^n)) \Bigg \rparen \\
      &\leq \frac{1}{n} \Bigg \lparen \sum_{x^n \in A_{\varepsilon}^{(n)}} \tilde{p}(x^n) (\lfloor nr \rfloor + 1) 
          + \sum_{x^n \in (A_{\varepsilon}^{(n)})^c} \tilde{p}(x^n) (\lfloor nr \rfloor + 2) \Bigg \rparen \\
      &= \frac{1}{n} \big \lparen P_{X^n}(A_{\varepsilon}^{(n)})  (\lfloor nr \rfloor + 1) 
        + P_{X^n}((A_{\varepsilon}^{(n)})^c) \rparen (\lfloor nr \rfloor + 2) \big \rparen \\
        &\leq \frac{1}{n} \left \lparen (1-\varepsilon) \cdot (\lfloor nr \rfloor + 1)  + \varepsilon (\lfloor nr \rfloor + 2)   \right \rparen \quad \because 2 
    \end{align*}
    \let\qedsymbol\relax
  \end{proof}
\end{frame}

\begin{frame}\frametitle{情報源符号化定理の証明}
  \begin{proof}[証明]
    \begin{align*}
      &= \frac{\lfloor nr \rfloor}{n} + \frac{1}{n} + \frac{\varepsilon}{n} \\
      &\leq r + \frac{1}{n} + \frac{\varepsilon}{n} \quad \because \lfloor nr \rfloor \leq nr \\
      &\leq r + \varepsilon^{\prime} \quad \left \lparen \varepsilon^{\prime} \coloneq \frac{1}{n} + \frac{\varepsilon}{n} \right \rparen
    \end{align*}
    ここで，小さい$\varepsilon$を取ったあとに，十分大きな$n$を取ることで，任意に$\varepsilon^{\prime}$を小さくできる．
    よって，$n \to \infty$の極限で$R = r$

    $r < H(X)$のときも同様に説明できる．
  \end{proof}
\end{frame}

\end{document}
